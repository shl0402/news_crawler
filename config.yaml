# =============================================================================
# Financial News Crawler Configuration
# =============================================================================

# Stock list grouped by category
stocks:
  Auto:
    - code: "1211.HK"
      name: "BYD"
  Tech:
    - code: "0700.HK"
      name: "Tencent"
    - code: "9988.HK"
      name: "Alibaba"
  Bank:
    - code: "0005.HK"
      name: "HSBC"
  Chips:
    - code: "0981.HK"
      name: "SMIC"

# Date range for news search
date_range:
  # Number of days to look back from today
  lookback_days: 14
  # Number of days per search period (default 7 = weekly)
  # Examples: 7 = weekly, 10 = every 10 days, 14 = bi-weekly, 30 = monthly
  period_days: 7

# Search settings
search:
  # Number of news articles to fetch per stock PER PERIOD
  # Total = results_per_period * (lookback_days / period_days)
  # Example: 2/period * (49 days / 7) = 14 articles per stock
  results_per_period: 2
  # Search language
  language: "en"
  # News sources to prioritize (used in search queries)
  preferred_sources:
    - "reuters"
    - "bloomberg"
    - "scmp"
    - "hkej"

# Request settings (to avoid getting banned)
request:
  # Delay between requests in seconds
  delay_seconds: 2
  # Timeout for requests in seconds
  timeout: 30
  # User agent for requests
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# Scraping settings
scraping:
  # Number of parallel Selenium WebDrivers (more = faster but uses more memory)
  # Recommended: 3-5 for most systems, up to 10 for high-end machines
  parallel_drivers: 5
  # Minimum word count for article content (articles below this are skipped)
  # Set to 350 to ensure quality content
  min_content_words: 350

# Output settings
output:
  # Output format: "excel", "jsonl", or "both"
  format: "both"
  # Output directory
  directory: "output"
  # Base filename (without extension)
  filename: "financial_news_data"

# Logging settings
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  # Log file path
  file: "crawler.log"
